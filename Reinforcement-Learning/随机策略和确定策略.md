# 随机策略和确定策略

## 随机策略

- $\pi_{\theta}(a | s)=P[a | s ; \theta]$ 在状态 $s$ 时，动作符合参数为 $\theta$ 的概率分布
- 采用随机策略时，即使在相同的状态，每次所采取的动作也很可能不一样
- 当采用高斯策略的时候，相同的策略，在同一个状态s处，采样的动作总体上看就算是不同，也差别不是很大，因为它们符合高斯分布，在均值附近的概率很大
- 随机策略将探索和改进集成到一个策略中

## 确定性策略

- $a=\mu _\theta (s)$ 
- 相同的策略（即 $\theta$ 相同时），在状态 s 时，动作是唯一确定的
- 确定性策略的优点：需要采样的数据少，算法效率高
- 当初试状态已知时，用确定性策略所产生的轨迹是固定
- 行动策略是随机策略，以保证充足的探索；评估策略是确定性策略。整个确定性策略的学习框架采用AC的方法

# DDPG

- We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. 
- 利用深度神经网络逼近行为值函数 $Q^w(s,a)$ 和确定性策略 $\mu_\theta(s)$ 

# ME-TRPO

- 

