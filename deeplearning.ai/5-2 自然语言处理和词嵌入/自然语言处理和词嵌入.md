> [åºåˆ—æ¨¡å‹](http://mooc.study.163.com/course/2001280005)
> å´æ©è¾¾ Andrew Ng

# Natural Language Processing & Word Embeddings

## Introduction to Word Embeddings

### Word Representation è¯æ±‡è¡¨å¾

- 1-hot representation
  - è¯æ˜¯å­¤ç«‹çš„ï¼Œä¹‹é—´æ²¡æœ‰å…³è”
  - ä»»æ„ä¸¤ä¸ªè¯å‘é‡çš„å†…ç§¯ä¸º0
  - ç›¸å…³è¯çš„æ³›åŒ–èƒ½åŠ›ä¸å¼º
- fearurized representation - word embedding è¯åµŒå…¥
  - ä¸€ä¸ªå•è¯ç”¨å¤šä¸ªç‰¹å¾ç»„æˆçš„å‘é‡æ¥è¡¨ç¤º more dense vector
  - t-SNEç®—æ³•ç”¨äºå¯è§†åŒ–ï¼ŒæŠŠé«˜ç»´å‘é‡æ˜ å°„åˆ°ä½ç»´ç©ºé—´

### Using word embeddings

- Learn word embeddings from large text corpus.
- Transfer embedding to new task with smaller training set.
- Continue to finetune the word embeddings with new data. (optional)
- æ¯ä¸€ä¸ªè¯æ±‡è¡¨çš„å•è¯çš„å›ºå®šåµŒå…¥ï¼Œå­¦ä¹ ä¸€ä¸ªå›ºå®šçš„ç¼–ç 
- äººè„¸è¯†åˆ«ä¸­çš„ç®—æ³•å¯èƒ½æ¶‰åŠåˆ°æµ·é‡çš„äººè„¸ç…§ç‰‡ï¼Œè€Œè‡ªç„¶è¯­è¨€å¤„ç†æœ‰ä¸€ä¸ªå›ºå®šçš„è¯æ±‡è¡¨ï¼Œä¸€äº›æ²¡æœ‰å‡ºç°è¿‡çš„å•è¯è®°ä¸ºæœªçŸ¥å•è¯

### Properties of word embeddings

- analogies using word vector ç±»æ¯”
- ä½™å¼¦ç›¸ä¼¼åº¦ cosine similarity, $sim({\bf u}, {\bf v})=\frac{\bf u^T \bf v}{\Vert \bf u\Vert _2\Vert\bf v\Vert _2}$ 
- éœ€è¦è¶³å¤Ÿå¤§çš„è¯­æ–™åº“

### Embedding matrix

- $E \cdot o_i = e_i$ å¯æå–æŸä¸€å•è¯çš„åµŒå…¥å‘é‡
- åœ¨å®è·µä¸­ä½¿ç”¨ä¸€ä¸ªä¸“é—¨çš„å‡½æ•°æ¥å•ç‹¬æŸ¥æ‰¾çŸ©é˜µğ¸çš„æŸåˆ—ï¼Œè€Œä¸æ˜¯ç”¨é€šå¸¸çš„çŸ©é˜µä¹˜æ³•æ¥åš

## Learning Word Embeddings: Word2vec & GloVe

### Learning word embeddings

- fixed historical window åªçœ‹å‰nä¸ªå•è¯æ¥é¢„æµ‹ä¸‹ä¸€ä¸ª

- netural language model

  ![neural language model](neural language model.png)

### Word2Vec

- skip-gram: æ ¹æ® context word é¢„æµ‹ target word

  ![skip-gram](skip-gram.png)

  $\theta_t:$ parameter associate with output t

- ç”¨é™„è¿‘çš„ä¸€ä¸ªå•è¯ä½œä¸ºä¸Šä¸‹æ–‡

- hierarchical softmax classifier åˆ†çº§softmaxåˆ†ç±»å™¨ï¼ŒåŠ é€Ÿåˆ†ç±»

- ç›®æ ‡è¯åˆ†å¸ƒå¹¶ä¸æ˜¯å•çº¯çš„åœ¨è®­ç»ƒé›†è¯­æ–™åº“ä¸Šå‡åŒ€ä¸”éšæœºçš„é‡‡æ ·å¾—åˆ°çš„ï¼Œè€Œæ˜¯é‡‡ç”¨äº†ä¸åŒçš„åˆ†çº§ï¼Œæ¥å¹³è¡¡å¸¸è§çš„è¯å’Œä¸å¸¸è§çš„è¯

- CBOW è¿ç»­è¯è¢‹æ¨¡å‹(Continuous Bag-Of-Words Model)æ˜¯ä»åŸå§‹è¯­å¥æ¨æµ‹ç›®æ ‡å­—è¯ï¼›Skip-Gram æ­£å¥½ç›¸åï¼Œæ˜¯ä»ç›®æ ‡å­—è¯æ¨æµ‹å‡ºåŸå§‹è¯­å¥

### Negative Sampling è´Ÿé‡‡æ ·

- context word, target word, label

- ä»å­—å…¸ä¸­éšæœºé€‰å–å…¶ä»–çš„è¯ï¼Œæ ‡è®°ä¸ºè´Ÿæ ·æœ¬

  ![è´Ÿé‡‡æ ·](è´Ÿé‡‡æ ·.png)

- è½¬æ¢ä¸ºäºŒåˆ†ç±»é—®é¢˜

### GloVe word vectors

- global vectors for word representation
- $X_{ij}$ is a count that captures how often do words i and j appear close to each other
- ![GloVe model](GloVe model.png)

## Applications using Word Embeddings

### Sentiment Classification æƒ…æ„Ÿåˆ†ç±»

- å–å¹³å‡å°±å¿½ç•¥äº†è¯­åº

  ![å–å¹³å‡](å–å¹³å‡.png)

- RNN

  ![RNN for sentiment](RNN for sentiment.png)

### Debiasing word embeddings è¯åµŒå…¥é™¤å

- SVD singular value decomposition å¥‡å¼‚å€¼åˆ†è§£