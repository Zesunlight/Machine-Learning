> [æ”¹å–„æ·±å±‚ç¥ç»ç½‘ç»œï¼šè¶…å‚æ•°è°ƒè¯•ã€æ­£åˆ™åŒ–ä»¥åŠä¼˜åŒ–](http://mooc.study.163.com/course/2001281003)
> å´æ©è¾¾ Andrew Ng

# æ·±åº¦å­¦ä¹ çš„å®ç”¨å±‚é¢

Setting up your ML application

## Train/dev/test sets (è®­ç»ƒã€å¼€å‘ã€æµ‹è¯•é›†)

- the goal of the dev set is to test different algorithms on it and see which algorithm works better
- test set is going to give you a pretty confident estimate of how well it's doing
- æ•°æ®é‡è¶Šå¤§ï¼Œè®­ç»ƒé›†æ‰€å æ¯”ä¾‹å°±è¶Šå¤§
- **Make sure dev set and test set come from the same distribution** 
- cross-validation äº¤å‰éªŒè¯ (dev set)

## Bias(åå·®) and Variance(æ–¹å·®)

- high bias: under fitting (high train set error)
- high variance: over fitting (low train set error but high dev set error)
- high bias and high variance: high train set error and higher dev set error
- optimal error (Bayes error) æœ€ä¼˜è¯¯å·®ä¹Ÿè¢«ç§°ä½œè´å¶æ–¯è¯¯å·®

## Basic "recipe" for machine learning

![basic recipe](basic recipe.png)

- ç½‘ç»œè§„æ¨¡å¤§å¾€å¾€å¯ä»¥é¿å…é«˜åå·®ï¼Œå»¶é•¿è®­ç»ƒæ—¶é—´å¯èƒ½æœ‰ç”¨å¯èƒ½æ²¡ç”¨
- æ›´å¤šçš„æ•°æ®å’Œæ­£åˆ™åŒ–å¯ä»¥å‡å°æ–¹å·®
- trade off between bias and variance

## Regularization (æ­£åˆ™åŒ–)

### Logistic Regression

$\lambda$ is regularization parameter (æ­£åˆ™åŒ–å‚æ•°), use **lambd** to represent teh lambda regularization parameter in code

#### L~2~ Regularization

- $J(w,b)=\frac{1}{m}\sum^m_{i=1}\mathcal L(\hat y^{(i)},y^{(i)})+\frac{\lambda }{2m}\|w\|^2_2$ 
- $\|w\|^2_2=\sum^{n_x}_{j=1}w_j^2=w^Tw$  
- æµ‹è¯•ä¸åŒçš„ $\lambda$ 

#### L~1~ regularization

-  $\frac{\lambda }{2m}\|w\|_1=\frac{\lambda }{2m}\sum^{n_x}_{j=1}|w_j|$ 
-  $w$ vector will have a lot of zeros in it, so make your model sparse

### Neural Network

- $\frac{\lambda }{2m}\sum^L_{l=1}\|w^{[l]}\|^2_F$ 

- $\|w^{[l]}\|^2=\sum^{n^{[l-1]}}_{i=1}\sum^{n^{[l]}}_{j=1}(w^{[l]}_{ij})^2$ (æ‰€æœ‰å…ƒç´ çš„å¹³æ–¹å’Œ)

- Frobenius Norm (å¼—ç½—è´å°¼ä¹Œæ–¯èŒƒæ•°)

- $dw^{[l]}=(back\; propagation)+\frac{\lambda}{m}w^{[l]}$ 

  $w^{[l]}=w^{[l]}-\alpha dw^{[l]}$ (weight decay)

- L~2~ èŒƒæ•°æ­£åˆ™åŒ–ä¹Ÿç§°ä¸ºæƒé‡è¡°å‡

## Regularization reduces over fitting

- $\lambda$ è¶³å¤Ÿå¤§æ—¶ï¼Œ$\mathbf w$ ä¼šæ¥è¿‘äº 0 
- reduce the impact of a lot of hidden units, so end up with a simper network
- ä¾‹å¦‚æ¿€æ´»å‡½æ•°æ˜¯$\tanh (z)$ ï¼Œ$\mathbf w$ è¾ƒå°æ—¶ï¼Œ$\mathbf z$ ä¹Ÿç›¸å¯¹å°
  - å‡½æ•°å‡ ä¹å‘ˆçº¿æ€§å…³ç³»
  - every layer will be roughly linear, just like a linear regression
  - if every layer is linear, then the whole network is just a linear network


## Dropout Regularization

![dropout](dropout.png)

- have a $p$ chance of keeping each node and $(1-p)$ chance of removing each node 
- eliminate some nodes, and remove all the ingoing outgoing lines form that node
- end up with a smaller and diminished network
- No dropout during test set

### inverted dropout (åå‘éšæœºå¤±æ´»)

![inverted dropout](inverted dropout.png)

- `keep_prob` ä¿ç•™æŸä¸ªç»“ç‚¹çš„æ¦‚ç‡
- by dividing by the keep-prob, it ensures that the expected value of $a^{[3]}$ remains the same
- æ¯ä¸€æ¬¡è¿­ä»£éƒ½ä½¿éšæœºçš„ä¸€äº›ç»“ç‚¹ç½®é›¶ï¼Œæ— è®ºæ­£å‘åå‘

## Understanding Dropout

- on every iteration, working with a smaller neural network
- can't rely on any one feature, any one of its inputs could go away at random
- æ”¶ç¼©æƒé‡çš„å¹³æ–¹èŒƒæ•° shrink the squared norm of the weights
- å‚æ•°å¤šçš„å±‚ `keep_prob` è®¾ç½®çš„å°ä¸€ç‚¹ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
- é€šå¸¸ä¸åœ¨è¾“å…¥å±‚åº”ç”¨ dropout
- dropout is very frequently used by computer vision
- cost function J is no longer well-defined, hard to calculated
- å…ˆä¸ç”¨ dropoutï¼Œç½‘ç»œçš„ cost æ˜¯é€’å‡çš„ï¼Œä¸”å‘ç”Ÿäº†è¿‡æ‹Ÿåˆï¼Œå†æ‰“å¼€ dropout


## Other Regularization Methods

### Date augmentation (æ•°æ®æ‰©å¢)

- flip horizontally æ°´å¹³ç¿»è½¬
- random crops of the image éšæœºè£å‰ª
- rotate æ—‹è½¬

### Early stopping

- éªŒè¯é›†çš„é”™è¯¯é€šå¸¸å¼€å§‹æ—¶ä¸‹é™ï¼Œ æŸä¸ªç‚¹åä¸Šå‡
- åœ¨é‚£ä¸ªç‚¹åœæ­¢è®­ç»ƒ stop the training of neural network earlier
- å‚æ•°åˆšå¼€å§‹æ˜¯æ¯”è¾ƒå°çš„ï¼Œéšç€è®­ç»ƒè¶Šæ¥è¶Šå¤§
- æå‰åœæ­¢è®­ç»ƒï¼Œcost function å¯èƒ½ä¸å¤Ÿå°

## Normalizing Inputs (å½’ä¸€åŒ–è¾“å…¥)

- zero out the mean (é›¶å‡å€¼åŒ–): just move the training set until it has 0 mean
- normalize the variances (å½’ä¸€åŒ–æ–¹å·®)ï¼Œä½¿æ–¹å·®ä¸º1ï¼Œä¼¸ç¼©å˜æ¢
- after normalizing features, cost function will on average look more symmetric(å¯¹ç§°)
- å¦‚æœç‰¹å¾å€¼çš„èŒƒå›´ç›¸å·®ä¸å¤§ï¼Œå½’ä¸€åŒ–ä¹Ÿå°±æ²¡å¤šé‡è¦
- åº”è¯¥å¯ä»¥åŠ é€Ÿç¥ç»ç½‘ç»œçš„è®­ç»ƒ


## Vanishing / Exploding Gradients (æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸)

- activations end up increasing/decreasing exponentially
- çº¿æ€§æ¿€æ´»å‡½æ•°çš„ä¾‹å­
  ![æ¢¯åº¦æ¶ˆå¤±çˆ†ç‚¸](æ¢¯åº¦æ¶ˆå¤±çˆ†ç‚¸.png)
- æ¿€æ´»å‡½æ•°çš„è¾“å…¥ç‰¹å¾è¢«é›¶å‡å€¼å’Œæ ‡å‡†æ–¹å·®åŒ–ï¼Œæ–¹å·®æ˜¯ 1ï¼Œ ğ‘§ ä¹Ÿä¼šè°ƒæ•´åˆ°ç›¸ä¼¼èŒƒå›´ï¼Œå¯ä»¥å‡å°‘æ¢¯åº¦çˆ†ç‚¸å’Œæ¶ˆå¤±

### Single Neuron Example

- æ¿€æ´»å‡½æ•°ç”¨ ReLU(z)ï¼Œåˆå§‹åŒ– $W^{[l]} = np.random.randn(shape) * np.sqrt(2 / n^{[l-1]} )$ 
- è‹¥ä½¿ç”¨ $\tanh(z)$ï¼Œç³»æ•°ä½¿ç”¨$np.sqrt(2 / n^{[l-1]} )$ æ›´å¥½ (Xavier åˆå§‹åŒ–)


## Gradient Checking (æ¢¯åº¦æ£€éªŒ)

### Numerical approximation of gradients

- two-sided difference is more accurate than one-sided difference
- $\frac{f(x+\epsilon)-f(x-\epsilon)}{2\epsilon}=\frac{1}{2}(\frac{f(x+\epsilon)-f(x)}{\epsilon}+\frac{f(x)-f(x-\epsilon)}{\epsilon})$ 

### Grad check

- take $W^{[1]},b^{[1]},...,W^{[L]},b^{[L]}$ and reshape to a big vector $\theta$ 
- take $dW^{[1]},db^{[1]},...,dW^{[L]},db^{[L]}$ and reshape to a big vector $d\theta$ 
- ![grad check](grad check.png)

### Implementation notes

- Don't use in training, only to debug
- Look at components to try to identify bug
- Remember to include regularization
- Doesn't work with dropout






